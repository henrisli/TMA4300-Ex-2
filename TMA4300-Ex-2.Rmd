--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(spam)         # load the data
attach(Oral)          # allow direct referencing to Y and E# load some libraries to generate nice map plots
library(fields, warn.conflict=FALSE)
library(colorspace)
col <- diverge_hcl(8)      # blue - red
# use a function provided by spam to plot the map together with the mortality rates
germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)
```

We wish to carry out a spatial analysis on mortality rates of oral cavity cancer in males in Germany during a 5-year period, 1986–1990.

We assume that the observed counts are conditionally independent Poisson

$$ \lambda_i \lvert \eta_i \sim Poisson(E_i \exp{\eta_i}) $$
The log relative risk $\boldsymbol{\eta} = (\eta_1, \dots, \eta_n)^T$ is decomposed into

$$ \boldsymbol{\eta} =  \mathbf{u} + \mathbf{v} $$, 

where the component $\mathbf{u} = (u_1, \dots, u_n)$ is spatially structured with smoothing parameter $\kappa_u$. The component $\mathbf{v} = (v_1, \dots, v_n)$ is unstructured white noise with precision parameter $\kappa_v$, i.e. $\mathbf{v} \sim N(\mathbf{0}, \kappa_v^{-1} I)$

For $\mathbf{u}$, we assume that neighboring districts are more similar than distant districts. We define that two districts are neighbours if they share a common border. $\mathbf{u}$ then becomes an intrinsic Gaussian Markov random field with density

$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \bigg (  \frac{-\kappa_u}{2}\sum_{i\sim j}{(u_i - u_j)^2}  \bigg)$$
We can define $R$ as the matrix
$$ R_{ij} = \begin{cases} n_i, \quad i = j \\
-1, \quad i \sim j \\
0, \quad \text{otherwise}. \end{cases}$$
and get
$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \Big (  \frac{-\kappa_u}{2}
\mathbf{u}^T R \mathbf{u}\Big)$$



Where $n_i$ is the number of neighboring districts of district $i$ and $i \sim j$ denotes that district $i$ is a neighboring district of district $j$. The distribution of $\boldsymbol{\eta}$, conditional on the spatial component $\mathbf{u}$ and $\kappa_v$, is 

$$ \boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v \sim \mathcal{N}(\mathbf{u}, \kappa^{-1} I).$$

The precision terms $\kappa_u$ and $\kappa_v$ are assigned Gamma priors

$$ \kappa_u \sim \text{Gamma}(\alpha_u, \beta_u), \\
\kappa_v \sim \text{Gamma}(\alpha_v, \beta_v).$$

With $\alpha_u = \alpha_v = 1$ and $\beta_u = \beta_v = 0.01$. 

We wish to analyse the data and its underlying spatial structure by implementing a Gibbs sampler with individual parameter  updates based on the full conditional  distributions. 

# Exercise 1

## a)

We start by computing the joint distribution from likelihood and prior distributions.

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \cdot p(\mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \boldsymbol{\eta} ) p(\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v) p(\kappa_v)p(\mathbf{u} \lvert \kappa_u )p(\kappa_u)    \\
 $$
Inserting the Gamma density for $\kappa_u$ and $\kappa_v$, the normal density for $\mathbf{u}\lvert \kappa_u$ and the normal density for $\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v$ we get 


\begin{align*}
&p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto\\
&p(\mathbf{y} \lvert \boldsymbol{\eta} ) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big)
\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u).
\end{align*}

Inserting the Poisson density of the data $\mathbf{y}$, we get 

\begin{align*}
p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto &\Big(\prod_{i=1}^n \exp(\eta_i)^{y_i} \exp(-E_i \exp(\eta_i))\Big) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big) \cdot\\
&\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u),
\end{align*}

which we can rewrite to 

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}
-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) + \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).$$

The posterior distribution of the parameters $p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$ is proportional to the joint distribution, giving us that 

\begin{align*} 
&p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y}) \propto p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto \\
&\kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) - \frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}+ \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).
\end{align*}

## b)
Due to the non-normality, sampling from the posterior distribution will require a Metropolis–Hastings step. To obtain a proposal distribution that is easy to sample from, here a Gaussian, we note that we can approximate the function 

$$f(\eta_i) = y_i \eta_i - E_i \exp(\eta_i)$$

by its 2nd order Taylor expansion, $\tilde{f}(\eta_i)$, around a point $z_i$. The approximation can be written as, 

\begin{align*}\tilde{f}(\eta_i) &= y_i z_i - E_i \exp(z_i) + (y_i - E_i \exp(z_i)) (\eta_i - z_i) - E_i \exp(z_i) \frac{(\eta_i - z_i)^2}{2} \\
&= E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2}) + \eta_i (y_i + E_i \exp(z_i)(z_i - 1)) - \frac{\eta_i^2}{2} E_i \exp (z_i) \\
&= a_i + b_i \eta_i - \frac{c_i}{2}\eta_i^2,
\end{align*}
where $a_i = E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2})$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$ and $c_i = E_i \exp (z_i)$.

## c)

The full conditional of $\kappa_u$ can be computed by

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) = \frac{p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})}{ \int p(\boldsymbol{\eta}, \mathbf{u}, \kappa_v, \kappa_u \lvert \mathbf{y})d\kappa_u} \propto  p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$$
We use the fact that any factor in the posterior not containing $\kappa_u$ are now constants and get

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y})\propto  p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y}) \propto \kappa_u^{\frac{n-1}{2} + \alpha_u - 1} \exp \big(- \kappa_u (\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u})\big), $$
which we recognize to be the density of a gamma-distributed variable with parameters $\frac{n-1}{2}+\alpha_u$ and $\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u}$.

We follow along the same lines for $\kappa_v$, and get 

$$p(\kappa_v \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_u, \mathbf{y}) \propto \kappa_v^{\frac{n}{2} + \alpha_v - 1} \exp \big(- \kappa_v (\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u}))\big),$$
which we recognize to be the density of a gamma-distributed variables with parameters $\frac{n}{2}+\alpha_v$ and $\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u})$.

Again, we do the same for $\mathbf{u}$, and get

$$p(\mathbf{u} \lvert \boldsymbol{\eta},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{\kappa_v}{2} \mathbf{u}^T \mathbf{u} - \frac{\kappa_u}{2} \mathbf{u}^T R \mathbf{u}\big) \\
= \exp(\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{1}{2}\mathbf{u}^T (\kappa_v I + \kappa_u R) \mathbf{u} \big),$$

which we recognize to be a normal density in canonical form with $\mathbf{b} = \kappa_v \boldsymbol{\eta}$ and $A = \kappa_v I + \kappa_u R$.

Finally, we repeat the procedure for $\boldsymbol{\eta}$, and get

$$p(\boldsymbol{\eta} \lvert \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{\kappa_v}{2}\boldsymbol{\eta}^T \boldsymbol{\eta} + \sum_i (y_i \eta_i - E_i \exp(\eta_i))\Big) \\
= \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{1}{2}\boldsymbol{\eta}^T (\kappa_v I) \boldsymbol{\eta} + \boldsymbol{\eta}^T\mathbf{y} - \exp(\boldsymbol{\eta})^T\mathbf{E}\Big)$$

Using the Taylor expansion of $\mathbf{y}^T \boldsymbol{\eta} - \exp(\boldsymbol{\eta})^T \mathbf{E}$, as described in b), we may insert this into $p(\boldsymbol{\eta} \lvert \cdot )$ to create the approximation $q(\boldsymbol{\eta}\lvert \mathbf{z}, \cdot)$,

$$ q(\boldsymbol{\eta} \lvert \mathbf{z}, \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\boldsymbol{\eta}^T(\kappa_v\mathbf{u} + \mathbf{b}) -\frac{1}{2}\boldsymbol{\eta}^T \big(\kappa_v I + \text{diag}(\mathbf{c})\big) \boldsymbol{\eta} \big ),$$

where $\mathbf{b} = (b_1, \dots, b_n)^T$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$, $\mathbf{c} = (c_1, \dots, c_n)^T$, $c_i = E_i \exp(z_i)$, and $\text{diag}(\mathbf{c})$ is the matrix with the elements of $\mathbf{c}$ on the diagonal and zeros elsewhere. 

We observe that $q(\boldsymbol{\eta} \lvert \cdot)$ is a normal density written in canonical form and thus easily sampled from. We also observe that as the $\exp(\cdot)$ is a continuous transform, $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ will converge to $p(\boldsymbol{\eta} \lvert \cdot)$ as $\mathbf{z}$ gets increasingly close to $\boldsymbol{\eta}$. Thus, for a reasonably good $\mathbf{z}$,  $q(\boldsymbol{\eta} \lvert \mathbf{z})$ will be a good choice for a proposal density for $p(\boldsymbol{\eta} \lvert \cdot)$. 

#Exercise 2

We implement a Gibbs sampling algorithm with individual parameters updates using the full conditional for $\kappa_u$, $\kappa_v$ and $\mathbf{u}$. We update $\boldsymbol{\eta}$ using Metropolis-Hastings with $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ as the proposal density. We set $\mathbf{z} = \boldsymbol{\eta}^{(m-1)}$

