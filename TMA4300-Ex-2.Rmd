--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
header-includes: \usepackage{float}
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(ggplot2)
library(coda)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE, fig.pos = 'H')
```



We wish to carry out a spatial analysis on mortality rates of oral cavity cancer in males in Germany during a 5-year period, 1986–1990.

We assume that the observed counts are conditionally independent Poisson

$$ \lambda_i \lvert \eta_i \sim Poisson(E_i \exp{\eta_i}) $$
The log relative risk $\boldsymbol{\eta} = (\eta_1, \dots, \eta_n)^T$ is decomposed into

$$ \boldsymbol{\eta} =  \mathbf{u} + \mathbf{v} $$, 

where the component $\mathbf{u} = (u_1, \dots, u_n)$ is spatially structured with smoothing parameter $\kappa_u$. The component $\mathbf{v} = (v_1, \dots, v_n)$ is unstructured white noise with precision parameter $\kappa_v$, i.e. $\mathbf{v} \sim N(\mathbf{0}, \kappa_v^{-1} I)$

For $\mathbf{u}$, we assume that neighboring districts are more similar than distant districts. We define that two districts are neighbours if they share a common border. $\mathbf{u}$ then becomes an intrinsic Gaussian Markov random field with density

$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \bigg (  \frac{-\kappa_u}{2}\sum_{i\sim j}{(u_i - u_j)^2}  \bigg)$$
```{r, echo = F, eval = T, fig.align = "center", out.width = "80%", fig.cap = "\\label{fig:smr} Standardised mortality rates (SMR)"}
set.seed(123)
library(spam)         # load the data
attach(Oral)          # allow direct referencing to Y and E# load some libraries to generate nice map plots

#Create Standardized Mortality Rates
Oral$SMR = Oral$Y / Oral$E

library(fields, warn.conflict=FALSE)
library(colorspace)
col <- diverge_hcl(8)      # blue - red
# use a function provided by spam to plot the map together with the mortality rates
germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)
```
We can define $R$ as the matrix
$$ R_{ij} = \begin{cases} n_i, \quad i = j \\
-1, \quad i \sim j \\
0, \quad \text{otherwise}. \end{cases}$$
and get
$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \Big (  \frac{-\kappa_u}{2}
\mathbf{u}^T R \mathbf{u}\Big),$$
where $n_i$ is the number of neighboring districts of district $i$ and $i \sim j$ denotes that district $i$ is a neighboring district of district $j$. The distribution of $\boldsymbol{\eta}$, conditional on the spatial component $\mathbf{u}$ and $\kappa_v$, is 

$$ \boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v \sim \mathcal{N}(\mathbf{u}, \kappa^{-1} I).$$

The precision terms $\kappa_u$ and $\kappa_v$ are assigned Gamma priors

$$ \kappa_u \sim \text{Gamma}(\alpha_u, \beta_u), \\
\kappa_v \sim \text{Gamma}(\alpha_v, \beta_v).$$

With $\alpha_u = \alpha_v = 1$ and $\beta_u = \beta_v = 0.01$. 

We wish to analyse the data and its underlying spatial structure by implementing a Gibbs sampler with individual parameter  updates based on the full conditional  distributions. 

# Exercise 1 (Derivations)

## a)

We start by computing the joint distribution from likelihood and prior distributions.

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \cdot p(\mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \boldsymbol{\eta} ) p(\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v) p(\kappa_v)p(\mathbf{u} \lvert \kappa_u )p(\kappa_u)    \\
 $$
Inserting the Gamma density for $\kappa_u$ and $\kappa_v$, the normal density for $\mathbf{u}\lvert \kappa_u$ and the normal density for $\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v$ we get 


\begin{align*}
&p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto\\
&p(\mathbf{y} \lvert \boldsymbol{\eta} ) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big)
\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u).
\end{align*}

Inserting the Poisson density of the data $\mathbf{y}$, we get 

\begin{align*}
p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto &\Big(\prod_{i=1}^n \exp(\eta_i)^{y_i} \exp(-E_i \exp(\eta_i))\Big) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big) \cdot\\
&\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u),
\end{align*}

which we can rewrite to 

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}
-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) + \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).$$

The posterior distribution of the parameters $p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$ is proportional to the joint distribution, giving us that 

\begin{align*} 
&p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y}) \propto p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto \\
&\kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) - \frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}+ \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).
\end{align*}

## b)
Due to the non-normality, sampling from the posterior distribution will require a Metropolis–Hastings step. To obtain a proposal distribution that is easy to sample from, here a Gaussian, we note that we can approximate the function 

$$f(\eta_i) = y_i \eta_i - E_i \exp(\eta_i)$$

by its 2nd order Taylor expansion, $\tilde{f}(\eta_i)$, around a point $z_i$. The approximation can be written as, 

\begin{align*}\tilde{f}(\eta_i) &= y_i z_i - E_i \exp(z_i) + (y_i - E_i \exp(z_i)) (\eta_i - z_i) - E_i \exp(z_i) \frac{(\eta_i - z_i)^2}{2} \\
&= E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2}) + \eta_i (y_i + E_i \exp(z_i)(z_i - 1)) - \frac{\eta_i^2}{2} E_i \exp (z_i) \\
&= a_i + b_i \eta_i - \frac{c_i}{2}\eta_i^2,
\end{align*}
where $a_i = E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2})$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$ and $c_i = E_i \exp (z_i)$.

## c)

The full conditional of $\kappa_u$ can be computed by

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) = \frac{p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})}{ \int p(\boldsymbol{\eta}, \mathbf{u}, \kappa_v, \kappa_u \lvert \mathbf{y})d\kappa_u} \propto  p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$$
We use the fact that any factor in the posterior not containing $\kappa_u$ are now constants and get

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) \propto \kappa_u^{\frac{n-1}{2} + \alpha_u - 1} \exp \big(- \kappa_u (\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u})\big), $$

which we recognize to be the density of a gamma-distributed variable with parameters $\frac{n-1}{2}+\alpha_u$ and $\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u}$.

We follow along the same lines for $\kappa_v$, and get 

$$p(\kappa_v \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_u, \mathbf{y}) \propto \kappa_v^{\frac{n}{2} + \alpha_v - 1} \exp \big(- \kappa_v (\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u}))\big),$$

which we recognize to be the density of a gamma-distributed variables with parameters $\frac{n}{2}+\alpha_v$ and $\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u})$.

Again, we do the same for $\mathbf{u}$, and get

$$p(\mathbf{u} \lvert \boldsymbol{\eta},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{\kappa_v}{2} \mathbf{u}^T \mathbf{u} - \frac{\kappa_u}{2} \mathbf{u}^T R \mathbf{u}\big) \\
= \exp(\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{1}{2}\mathbf{u}^T (\kappa_v I + \kappa_u R) \mathbf{u} \big),$$

which we recognize to be a normal density in canonical form with $\mathbf{b} = \kappa_v \boldsymbol{\eta}$ and $A = \kappa_v I + \kappa_u R$.

Finally, we repeat the procedure for $\boldsymbol{\eta}$, and get

$$p(\boldsymbol{\eta} \lvert \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{\kappa_v}{2}\boldsymbol{\eta}^T \boldsymbol{\eta} + \sum_i (y_i \eta_i - E_i \exp(\eta_i))\Big) \\
= \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{1}{2}\boldsymbol{\eta}^T (\kappa_v I) \boldsymbol{\eta} + \boldsymbol{\eta}^T\mathbf{y} - \exp(\boldsymbol{\eta})^T\mathbf{E}\Big)$$

Using the Taylor expansion of $\mathbf{y}^T \boldsymbol{\eta} - \exp(\boldsymbol{\eta})^T \mathbf{E}$, as described in b), we may insert this into $p(\boldsymbol{\eta} \lvert \cdot )$ to create the approximation $q(\boldsymbol{\eta}\lvert \mathbf{z}, \cdot)$,

$$ q(\boldsymbol{\eta} \lvert \mathbf{z}, \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\boldsymbol{\eta}^T(\kappa_v\mathbf{u} + \mathbf{b}) -\frac{1}{2}\boldsymbol{\eta}^T \big(\kappa_v I + \text{diag}(\mathbf{c})\big) \boldsymbol{\eta} \big ),$$

where $\mathbf{b} = (b_1, \dots, b_n)^T$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$, $\mathbf{c} = (c_1, \dots, c_n)^T$, $c_i = E_i \exp(z_i)$, and $\text{diag}(\mathbf{c})$ is the matrix with the elements of $\mathbf{c}$ on the diagonal and zeros elsewhere. 

We observe that $q(\boldsymbol{\eta} \lvert \cdot)$ is a normal density written in canonical form and thus easily sampled from. We also observe that as the $\exp(\cdot)$ is a continuous transform, $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ will converge to $p(\boldsymbol{\eta} \lvert \cdot)$ as $\mathbf{z}$ gets increasingly close to $\boldsymbol{\eta}$. Thus, for a reasonably good $\mathbf{z}$,  $q(\boldsymbol{\eta} \lvert \mathbf{z})$ will be a good choice for a proposal density for $p(\boldsymbol{\eta} \lvert \cdot)$. 

# Exercise 2 (Implementation of the MCMC sampler)

We implement a Gibbs sampling algorithm with individual parameter updates using the full conditional for $\kappa_u$, $\kappa_v$ and $\mathbf{u}$. We update $\boldsymbol{\eta}$ using Metropolis-Hastings with $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ as the proposal density. We set $\mathbf{z} = \boldsymbol{\eta}^{(m-1)}$, and do $N = 50000$ iterations.


```{r, echo = T, eval = T}
# Setup
load("tma4300_ex2_Rmatrix.Rdata")
n = length(Oral$SMR)
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01
```


```{r, echo = T, eval = T}
source("dmvnorm.R")

sample_u = function(kappa_v, kappa_u, eta){
  #Samples from the canonical normal
  b = kappa_v * eta
  A = kappa_v * diag.spam(n) + kappa_u * R
  res = rmvnorm.canonical(1, b, A)
  return(t(res))
}

sample_eta = function(kappa_v, u, z){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_arg_sampler = kappa_v * u + b_taylor
  A_arg_sampler = kappa_v * diag.spam(n) + diag(c_taylor)
  
  return(as.vector(rmvnorm.canonical(1, b_arg_sampler, A_arg_sampler)))
}

log_full_conditional_eta = function(eta, kappa_v, u, log=T){
  res = kappa_v * eta %*% u - 1/2 * t(eta) %*% (kappa_v * diag.spam(n)) %*% eta + eta%*% Oral$Y - t(exp(eta))%*%Oral$E
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

log_full_conditional_eta_approx = function(eta, z, u, kappa_v,  log=T){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_canonical = kappa_v * u + b_taylor
  A_canonical = kappa_v * diag.spam(n) + diag.spam(c_taylor)
  
  #SKRIVE INN DMVNORM HER
  #res = eta %*% b_canonical - 1/2 * t(eta) %*% A_canonical %*% eta
  res = dmvnorm.canonical(eta, b_canonical, A_canonical)
  
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

#Set number of iterations
N_iter = 500

#Allocate vectors and arrays 
u = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
eta = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
kappau = vector(mode = "double", length = N_iter)
kappav = vector(mode = "double", length = N_iter)
updated_eta = vector(length = N_iter)
accept_prob = vector(length = N_iter)
times = vector(mode = "double", length = N_iter)

#Set start values
kappau_start = alpha_u / beta_u
kappav_start = alpha_v / beta_v
u_start = rep(0, length(Oral$SMR))
eta_start = log(Oral$SMR)

u[1,] = u_start
eta[1,] = eta_start
kappau[1] = kappau_start
kappav[1] = kappav_start



#Run the MCMC algorithm
for (i in 2:N_iter) {
  # Initialize clock to measure time
  t0 = as.numeric(Sys.time())
  # Sample kappau
  kappau[i] = rgamma(1, (n-1)/2+alpha_u, beta_u + 1/2 * t(u[i-1,])%*%R%*%u[i-1,])
  
  # Sample kappav
  kappav[i] = rgamma(1, n/2 + alpha_v, beta_v + 1/2 * t(eta[i-1,] - u[i-1,])%*%(eta[i-1,] - u[i-1,]))
  
  # Sample u
  res = sample_u(kappav[i], kappau[i], eta[i-1,])
  u[i, ] = res
  
  # Sample eta
  z = eta[i-1,]
  eta_prop = sample_eta(kappav[i], u[i,], z)
  
  logp1 = log_full_conditional_eta(eta_prop, kappav[i], u[i,])
  logp2 = log_full_conditional_eta(z, kappav[i], u[i,])
  logp3 = log_full_conditional_eta_approx(z, eta_prop, u[i,], kappav[i])
  logp4 = log_full_conditional_eta_approx(eta_prop, z, u[i,], kappav[i])
  
  # Compute acceptance prob
  log_alpha = min(0, logp1 - logp2 + logp3 - logp4)
  accept_prob[i] = exp(log_alpha)
  
  # Accept if r<alpha, with r coming from a uniform[0,1] distribution
  r = runif(1)
  if (log(r) < log_alpha){
    updated_eta[i] = TRUE
    eta[i,] = eta_prop
  }
  else{
    eta[i,] = eta[i-1,]
  }
  # Compute time elapsed in this iteration
  times[i] = as.numeric(Sys.time()) - t0 
}
```
We note that the code takes `r round(sum(times))` seconds, or `r round(sum(times)/60,1)` minutes.

# Exercise 3 (Convergence diagnostics)

We now check a few diagnostic summaries for the precision parameters $\kappa_u$ and $\kappa_v$. We also do the same for three randomly chosen components of $\mathbf{u}$ and $\mathbf{v}$.

First, we show a trace plot of $\kappa_u$. The values in the burn-in period are really large, so we display both a plot of the whole chain, and a plot where the first 500 values are removed.
```{r, echo = T, eval = F, out.width = "50%"}
df1 = data.frame(kappa_u = kappau, N = 1:N_iter)
ggplot(df1, aes(x = N, y = kappa_u)) + geom_point()
df12 = data.frame(kappa_u = kappau[500:N_iter], N = 500:N_iter)
ggplot(df12, aes(x = N, y = kappa_u)) + geom_point()
```

```{r, echo = F, eval = T, out.width = "50%"}
df1 = data.frame(kappa_u = kappau, N = 1:N_iter)
ggplot(df1, aes(x = N, y = kappa_u)) + geom_point()
df12 = data.frame(kappa_u = kappau[500:N_iter], N = 500:N_iter)
ggplot(df12, aes(x = N, y = kappa_u)) + geom_point()
```
From the second plot, we suspect that the burn-in period might be somewhat over 500 iterations.

We then display an autocorrelation plot for $\kappa_u$.
```{r, echo = T, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:acfku}Autocorrelation plot for kappa u"}
acf(kappau, lag.max = 60)
```
From the plot, we see that we can use only every $\sim 45$^th^ iteration to get a correlation which is not statistically significant. Taking into account the burn-in period, we would then only get about 200 uncorrelated samples from our Markov chain of size $N=50000$.

To conclude our diagnosis of the chain for $\kappa_u$, we check the chain for convergence by using the function `geweke.diag()` from the R package `coda`. The function provides a $z$-value, which we use to compute a corresponding $p$-value. Large $p$-values indicates that the chain has converged during the first $10\%$ iterations.
```{r, echo = T, eval = F}
gw = geweke.diag(kappau)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
```{r, echo = F, eval = T}
gw = geweke.diag(kappau)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
A $p$-value of `r round(2*(1-pnorm(abs(gw$z))),2)` indicates that the chain did converge during the first $10\%$ iterations, because this value is above any reasonable significance level.

We then move on to $\kappa_v$, and display both a plot of the whole chain, and a plot where the first 500 values are removed.
```{r, echo = T, eval = F, out.width = "50%"}
df2 = data.frame(kappa_v = kappav, N = 1:N_iter)
ggplot(df2, aes(x = N, y = kappa_v)) + geom_point()
df21 = data.frame(kappa_v = kappav[500:N_iter], N = 500:N_iter)
ggplot(df21, aes(x = N, y = kappa_v)) + geom_point()
```
```{r, echo = F, eval = T, out.width = "50%"}
df2 = data.frame(kappa_v = kappav, N = 1:N_iter)
ggplot(df2, aes(x = N, y = kappa_v)) + geom_point()
df21 = data.frame(kappa_v = kappav[500:N_iter], N = 500:N_iter)
ggplot(df21, aes(x = N, y = kappa_v)) + geom_point()
```
Also here, we suspect that the burn-in period might be somewhat over 500 iterations.

We then display an autocorrelation plot for $\kappa_v$.
```{r, echo = T, eval = F, out.width = "50%"}
acf(kappav)
acf(kappav, lag.max = 1000)
```

```{r, echo = F, eval = T, out.width = "50%"}
acf(kappav)
acf(kappav, lag.max = 1000)
```
The plot shows that the samples of $\kappa_v$ are highly correlated, which is not ideal.


As a final diagnostic for $\kappa_v$ we conduct Geweke's Diagnostic.
```{r, echo = T, eval = F}
gw = geweke.diag(kappav)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
```{r, echo = F, eval = T}
gw = geweke.diag(kappav)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
A $p$-value of `r round(2*(1-pnorm(abs(gw$z))),2)` indicates that the chain converged during the first $10\%$ iterations.

We then draw three random components of $\mathbf{u}$ and $\boldsymbol \eta$ and plot trace plots, autocorrelation functions and use the function `geweke.diag()` to check for convergence.
```{r, echo = T, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:3u}Trace plot for three random components of u"}
random_indexes = sort(sample(1:length(Oral$Y), 3, replace=F))
df4 = data.frame()
for (i in 1:3){
  index = random_indexes[i]
  df = data.frame(u = u[, index], N = 1:N_iter, shape = as.factor(i), col=as.factor(index))
  df4 = rbind(df4, df)
}
ggplot(df4, aes(x = N, y = u, shape = shape, col=col)) + geom_point() + scale_shape(solid=F) + guides(shape = F)
```
The trace plot for three components of $\mathbf{u}$ in figure \ref{fig:3u} looks correct, with a small burn-in period.

We then plot an autocorrelation plot for the same three components.
```{r, echo = T, eval = F, out.width = "33%"}
df5 = data.frame(u = u[,random_indexes])
acf(u[,random_indexes[1]], lag.max = 200)
acf(u[,random_indexes[2]], lag.max = 200)
acf(u[,random_indexes[3]], lag.max = 200)
```
```{r, echo = F, eval = T, out.width = "33%"}
df5 = data.frame(u = u[,random_indexes])
acf(u[,random_indexes[1]], lag.max = 200, main = paste0("ACF of u[",random_indexes[1],"]"))
acf(u[,random_indexes[2]], lag.max = 200, main = paste0("ACF of u[",random_indexes[2],"]"))
acf(u[,random_indexes[3]], lag.max = 200, main = paste0("ACF of u[",random_indexes[3],"]"))
```
The three components of $\mathbf{u}$ show varying degree of autocorrelation, but generally, all three components are somewhat autocorrelated, although less correlated than $\kappa_v$.

As a final diagnostic, we conduct Geweke's Diagnostic to the three components.
```{r, echo = T, eval = F}
gw = geweke.diag(df5)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
```{r, echo = F, eval = T}
gw = geweke.diag(df5)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
$p$-values of `r cat(round(2*(1-pnorm(abs(gw$z))),2))` indicate that the chain did converge during the first $10\%$ iterations only for component 7 and 424, and not for component 257.


```{r, echo = T, eval = T, out.width = "50%", fig.align = "center", fig.cap = "\\label{fig:3eta}Trace plot for three random components of eta"}
df4 = data.frame()
for (i in 1:3){
  index = random_indexes[i]
  df = data.frame(eta = eta[, index], N = 1:N_iter, shape = as.factor(i), col=as.factor(index))
  df4 = rbind(df4, df)
}
ggplot(df4, aes(x = N, y = eta, shape = shape, col=col)) + geom_point() + scale_shape(solid=F)+ guides(shape = F)
```
The trace plot of the three components of $\boldsymbol \eta$ in figure \ref{fig:3eta} looks correct, with a small burn-in period.

```{r, echo = T, eval = F}
df7 = data.frame(eta = eta[,random_indexes])
acf(eta[,random_indexes[1]], lag.max = 200)
acf(eta[,random_indexes[2]], lag.max = 200)
acf(eta[,random_indexes[3]], lag.max = 200)
```

```{r, echo = F, eval = T, out.width = "33%"}
df7 = data.frame(eta = eta[,random_indexes])
acf(eta[,random_indexes[1]], lag.max = 200, main = paste0("ACF of eta[",random_indexes[1],"]"))
acf(eta[,random_indexes[2]], lag.max = 200, main = paste0("ACF of eta[",random_indexes[2],"]"))
acf(eta[,random_indexes[3]], lag.max = 200, main = paste0("ACF of eta[",random_indexes[3],"]"))
```

Same as for the components of $\mathbf{u}$, the three components of $\boldsymbol \eta$ show varying degree of autocorrelation, but generally, all three components are highly autocorrelated.

Finally, the Geweke Diagnostic for the three components of $\boldsymbol \eta$ shows the following.
```{r, echo = T, eval = F}
gw = geweke.diag(df7)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
```{r, echo = F, eval = T}
gw = geweke.diag(df7)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```
$p$-values of `r cat(round(2*(1-pnorm(abs(gw$z))),2))` indicate that the chain converges during the first $10\%$ iterations for only component 7, and not for component 257 and 424.

SI NOE OM BURN-IN HER OGSÅ. All in all, we have seen that most of the variables in our Markov chain converge during the first $10\%$ iterations. Still, most of the samples are highly autocorrelated, which is a critical flaw. High autocorrelation decreases performance of the MCMC algorithm, implying that we have to run several iterations of the algorithm to produce the equivalent of an independent sample. Despite this weakness, the chain has in fact converged, and we believe it can be used.

# Exercise 4 (Effective sample size)
To assess the quality of the samples from our Markov chain, we calculate the effective sample size for the precision parameters $\kappa_u$ and $\kappa_v$ by using the function `effectiveSize()` from the `coda` library.

```{r, echo = T, eval = F}
running_time = sum(times)
cat("ESS of kappa u: ", effectiveSize(kappau), "\n")
cat("ESS of kappa_v: ", effectiveSize(kappav))
```
```{r, echo = F, eval = T}
cat("ESS of kappa u: ", effectiveSize(kappau), "\n")
cat("ESS of kappa_v: ", effectiveSize(kappav))
```
The ESS is the estimated number of independent samples needed to obtain a parameter estimate with the same precision as the MCMC estimate based on $N$ dependent samples. In our case, $N=50000$, which means that the quality of our $\kappa_u$ sample is as good as `r round(effectiveSize(kappau))` independent samples. Also, our $\kappa_v$ sample is as good as `r round(effectiveSize(kappav))` independent samples. The $\kappa_u$ samples are better than the $\kappa_v$ samples, which also could be seen by the fact that the $\kappa_u$ samples are much less correlated than the $\kappa_v$ samples.

How could the MCMC sampler be changed to possibly improve the ESS values??!?!?!?!?!

We also compute the relative ESS, which is the ESS divided by the running time of the MCMC algorithm. The relative ESS can be interpreted as a measure for the number of independent samples produced by the algorithm per second. We get the following values for the relative ESS of $\kappa_u$ and $\kappa_v$.
```{r, echo = T, eval = F}
running_time = sum(times)

cat("Relative ESS of kappa u: ", effectiveSize(kappau) / running_time, "\n")
cat("Relative ESS of kappa v: ", effectiveSize(kappav) / running_time)
```
```{r, echo = F, eval = T}
running_time = sum(times)

cat("Relative ESS of kappa u: ", effectiveSize(kappau) / running_time, "\n")
cat("Relative ESS of kappa v: ", effectiveSize(kappav) / running_time)
```
This means that the MCMC algorithm produces the equivalent of `r effectiveSize(kappau) / running_time` independent samples for $\kappa_u$ per second, and the equivalent of `r effectiveSize(kappav) / running_time` independent samples for $\kappa_v$ per second. Also, to get one independent sample of $\kappa_u$, we need to run the algorithm for `r round(running_time/effectiveSize(kappau),2)` seconds, while to get one independent sample of $\kappa_v$, we need to run the algorithm for `r round(running_time/effectiveSize(kappav),2)` seconds.

# Exercise 5 (Interpretation of results)
Finally, we plot the posterior median of $\exp(\mathbf{u})$ for all regions using the function `germany.plot()`. 

```{r, echo = T, eval = F, out.width = "50%", fig.align = "center"}
post_median_u = apply(exp(u[100:N_iter,]), 2, median)

germany.plot(post_median_u, col=col, legend=TRUE)
```

```{r, echo = F, eval = T, out.width = "50%", fig.cap = "\\label{fig:medianexpu} Posterior median of exp(u) for all regions in Germany"}
post_median_u = apply(exp(u[100:N_iter,]), 2, median)

germany.plot(post_median_u, col=col, legend=TRUE, main = "Median exp(u)")
```
We see from figure \ref{fig:medianexpu} that we have some spatial structure on the variable $\mathbf{u}$. We get high values of $\exp(\mathbf{u})$ in the south-west regions and somewhat high values in regions in the north-east. Additionally, we get small values of $\exp(\mathbf{u})$ in the south-east, north-west, north and middle-east around the cities of Munich, Düsseldorf, Kiel and Leipzig. We also see some clear deviation from the standardised mortality rates in figure \ref{fig:smr}, especially in the south-west. $\exp(\mathbf{u})$ is also smoother than the SMR from figure \ref{fig:smr}



# Exercise 6 (Comparison of INLA and inclusion of covariate information)

## a)

```{r, echo=T, eval=T}
library(spam)
attach(Oral)

# Create a copy of the dataframe Oral
dfinla = data.frame(Oral)
dfinla$region = seq(1, dim(Oral)[1])
  
# Copy the column region
dfinla$region2 = dfinla$region
```

```{r, echo=T, eval=T}
library(INLA)
g = system.file("demodata/germany.graph", package="INLA")

alpha_u = 1
beta_u = 0.01
alpha_v = 1
beta_v = 0.01


# Create kappau prior
kappauprior = list(prior="loggamma", param=c(alpha_u, beta_u))
# Create kappav prior
kappavprior = list(prior="loggamma", param=c(alpha_v, beta_v))

# eta = u(region) + v(region)
### , constr=F)
formula_1 <- Y~f(region,model="besag", graph = g, hyper=list(prec=kappauprior), constr=T) + 
            f(region2, model="iid", hyper=list(prec=kappavprior))

formula_0 <- Y~f(region,model="besag", graph = g, hyper=list(prec=kappauprior), constr=F) + 
            f(region2, model="iid", hyper=list(prec=kappavprior))

# Oral$Y = Oral$E * eta
result_1 <- inla(formula_1, family="poisson", E = dfinla$E, data=dfinla,
                 control.compute = list(dic=TRUE))
result_0 <- inla(formula_0, family="poisson", E = dfinla$E, data=dfinla,
                 control.compute = list(dic=TRUE))
```

```{r}
result = result_0
result_improved = inla.hyperpar(result)
```

```{r}
germany.plot(exp(result$summary.random$region$mean) - post_median_u, col=col, legend=TRUE)
```

```{r, echo=T, eval=T}
exp(result$summary.random$region$mean[random_indexes])
post_median_u[random_indexes]
```

```{r, echo=T, eval=T}
df60 = data.frame(result$marginals.hyperpar$`Precision for region`)
df61 = data.frame(x = kappau[1000:length(kappau)])
df62 = data.frame(result_improved$marginals.hyperpar$`Precision for region`)

ggplot() + geom_histogram(data=df61, aes(x = x, y = ..density.., col="MCMC"), bins=100) + geom_line(data=df60, aes(x = x, y = y, col="INLA")) + geom_line(data=df62, aes(x = x, y = y, col="INLA - improved")) + xlim(6, 34)
```


```{r, echo=T, eval=T}
df62 = data.frame(result$marginals.hyperpar$`Precision for region2`)
df63 = data.frame(x = kappav[1000:length(kappav)])
df64 = data.frame(result_improved$marginals.hyperpar$`Precision for region2`)

ggplot() + geom_histogram(data=df63, aes(x = x, y = ..density.., col="MCMC"), bins=100) + geom_line(data=df62, aes(x = x, y = y, col="INLA")) + geom_line(data=df64, aes(x = x, y = y, col="INLA - improved")) + xlim(40, 500)
```

```{r, fig.asp=0.5, fig.ncol = 1, fig.cap="output",fig.align = "center"}
df650 = data.frame(result$marginals.random$region[index])
df651 = data.frame(x = u[, index])
df652 = data.frame(result_improved$marginals.random$region[index])
  
colnames(df650) = c("x", "y")
colnames(df652) = c("x", "y")

index = random_indexes[1]
p1 = ggplot() + geom_histogram(data=df651, aes(x = x, y = ..density.., col="MCMC"), bins=100) + geom_line(data=df650, aes(x = x, y = y, col="INLA")) + geom_line(data=df652, aes(x = x, y = y, col="INLA - improved")) + xlab(paste("u[", index, "]", sep=""))

df660 = data.frame(result$marginals.random$region[index])
df661 = data.frame(x = u[, index])
df662 = data.frame(result_improved$marginals.random$region[index])
  
colnames(df660) = c("x", "y")
colnames(df662) = c("x", "y")

index = random_indexes[2]
p2 = ggplot() + geom_histogram(data=df661, aes(x = x, y = ..density.., col="MCMC"), bins=100) + geom_line(data=df660, aes(x = x, y = y, col="INLA")) + geom_line(data=df662, aes(x = x, y = y, col="INLA - improved")) + xlab(paste("u[", index, "]", sep=""))

df670 = data.frame(result$marginals.random$region[index])
df671 = data.frame(x = u[, index])
df672 = data.frame(result_improved$marginals.random$region[index])
  
colnames(df670) = c("x", "y")
colnames(df672) = c("x", "y")

index = random_indexes[3]
p3 = ggplot() + geom_histogram(data=df671, aes(x = x, y = ..density.., col="MCMC"), bins=100) + geom_line(data=df670, aes(x = x, y = y, col="INLA")) + geom_line(data=df672, aes(x = x, y = y, col="INLA - improved")) + xlab(paste("u[", index, "]", sep=""))

p1
p2
p3
```

DO THE SAME FOR ETA

## b)

```{r}
smoking = read.table("smoking.dat")
dfinla2 = data.frame(dfinla)
dfinla2$smoking = as.vector(smoking)$V1

formula_2 <- Y~f(region,model="besag", graph = g, hyper=list(prec=kappauprior)) + 
            f(region2, model="iid", hyper=list(prec=kappavprior)) + 
            f(smoking, model="linear")

result_2 = inla(formula_2, family="poisson", E = dfinla2$E, data=dfinla2,
                control.compute = list(dic=TRUE))

formula_3 <-  Y~f(region,model="besag", graph = g, hyper=list(prec=kappauprior)) + 
            f(region2, model="iid", hyper=list(prec=kappavprior)) + 
            f(smoking, model="rw2")

result_3 <- inla(formula_3, family="poisson", E = dfinla2$E, data=dfinla2,
                control.compute = list(dic=TRUE))

```

```{r}
cat("model w/o smoking", result_0$dic$dic, "\n")
cat("model w/ smoking", result_2$dic$dic, "\n")
cat("model w/ smoking RW2", result_3$dic$dic)

```

```{r}
df70 = data.frame(id = seq(1, length(result_3$summary.random$smoking$mean)),
                  mean =  result_3$summary.random$smoking$mean,
                  bot025 = result_3$summary.random$smoking$`0.025quant`,
                  top975 = result_3$summary.random$smoking$`0.975quant`)

ggplot() + geom_ribbon(data=df70, aes(x = id, ymin=bot025,ymax=top975)) +
  geom_line(data=df70, aes(x = id, y = mean, col="mean"), size=2) +
  geom_line(data=df70, aes(x = id, y = bot025, col="bot025"), size = 2) + 
  geom_line(data=df70, aes(x = id, y = top975, col="top975"), size=2) + 
  ylab("f(smoking)") + xlab("smoking")
  



```

