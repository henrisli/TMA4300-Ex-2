--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r}
library(spam)         # load the data
str(Oral)             # see structure of data
#’data.frame’: 544 obs. of  3 variables:
# $ Y  : int  18 62 44 12 18 27 20 29 39 21 . . .
# $ E  : num  16.4 45.9 44.7 16.3 26.9 . . .
# $ SMR: num  1.101 1.351 0.985 0.735 0.668 . . .
attach(Oral)          # allow direct referencing to Y and E# load some libraries to generate nice map plots
library(fields, warn.conflict=FALSE)
library(colorspace)
col <- diverge_hcl(8)      # blue - red
# use a function provided by spam to plot the map together with the mortality rates
germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)
```

We wish to carry out a spatial analysis on mortality rates of oral cavity cancer in males in Germany during a 5-year period, 1986–1990.

We assume that the observed counts are conditionally independent Poisson

$$ \lambda_i \lvert \eta_i \sim Poisson(E_i \exp{\eta_i}) $$
The log relative risk $\boldsymbol{\eta} = (\eta_1, \dots, \eta_n)^T$ is decomposed into

$$ \boldsymbol{\eta} =  \mathbf{u} + \mathbf{v} $$, 

where the component $\mathbf{u} = (u_1, \dots, u_n)$ is spatially structured with smoothing parameter $\kappa_u$. The component $\mathbf{v} = (v_1, \dots, v_n)$ is unstructured white noise with precision parameter $\kappa_v$, i.e. $\mathbf{v} \sim N(\mathbf{0}, \kappa_v^{-1} I)$

For $\mathbf{u}$, we assume that neighboring districts are more similar than distant districts. We define that two districts are neighbours if they share a common border. $\mathbf{u}$ then becomes an intrinsic Gaussian Markov random field with density

$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \bigg (  \frac{-\kappa_u}{2}\sum_{i\sim j}{(u_i - u_j)^2}  \bigg)$$
We can define $R$ as the matrix
$$ R_{ij} = \begin{cases} n_i, \quad i = j \\
-1, \quad i \sim j \\
0, \quad \text{otherwise}. \end{cases}$$
and get
$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \Big (  \frac{-\kappa_u}{2}
\mathbf{u}^T R \mathbf{u}\Big)$$



Where $n_i$ is the number of neighboring districts of district $i$ and $i \sim j$ denotes that district $i$ is a neighboring district of district $j$. The distribution of $\boldsymbol{\eta}$, conditional on the spatial component $\mathbf{u}$ and $\kappa_v$, is 

$$ \boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v \sim \mathcal{N}(\mathbf{u}, \kappa^{-1} I).$$

The precision terms $\kappa_u$ and $\kappa_v$ are assigned Gamma priors

$$ \kappa_u \sim \text{Gamma}(\alpha_u, \beta_u), \\
\kappa_v \sim \text{Gamma}(\alpha_v, \beta_v).
$$

With $\alpha_u = \alpha_v = 1$ and $\beta_u = \beta_v = 0.01$. 

We wish to analyse the data and its underlying spatial structure by implementing a Gibbs sampler with individual parameter  updates based on the full conditional  distributions. 

# Exercise 1

## a)

We start by computing the joint from our priors and the likelihood.

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \cdot p(\mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \cdot ) p(\kappa_u) p(\kappa_v) p(\mathbf{u} \lvert \kappa_u ) p(\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v) \\
 $$
Inserting the Gamma density for $\kappa_u$ and $\kappa_v$, the normal density for $\mathbf{u}\lvert \kappa_u$ and the normal density for $\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v$ we get 


$$p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto p(\mathbf{y} \lvert \cdot ) \kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u)\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big) 
\kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big)
$$

Inserting the Poisson density of the data $\mathbf{y}$, we get 

$$p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \Big(\prod_{i=1}^n \exp(\eta_i)^{y_i} \exp(-E_i \exp(\eta_i))\Big)  \kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u)\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big) 
\kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big), $$

which we can rewrite to 

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}
-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) + \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).$$

The posterior of the parameters $p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$ is proportional to the joint, giving us that 

$$ p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y}) \propto p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}
-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) + \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).$$

## b)

Before proceeding, we note that we can approximate the function 

$$f(\eta_i) = y_i \eta_i - E_i \exp(\eta_i)$$

by its 2nd order Taylor expansion around $z_i$, 

$$\tilde{f}(\eta_i) = y_i z_i - E_i \exp(z_i) + (y_i - E_i \exp(z_i)) (\eta_i - z_i) - E_i \exp(z_i) \frac{(\eta_i - z_i)^2}{2} \\
= E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2}) + \eta_i (y_i + E_i \exp(z_i)(z_i - 1)) - \frac{\eta_i^2}{2} E_i \exp (z_i) \\
= a_i + b_i \eta_i - \frac{c_i}{2}\eta_i^2,$$

where $a_i = E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2})$, $b_i = (y_i + E_i \exp(z_i)(z_i - 1))$, $c_i = E_i \exp (z_i)$.

## c)

The full conditional of $\kappa_u$ can be computed by

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) = \frac{p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})}{ \int p(\boldsymbol{\eta}, \mathbf{u}, \kappa_v, \kappa_u \lvert \mathbf{y})d\kappa_u} \propto  p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$$
We use the fact that any factor in the posterior not containing $\kappa_u$ are now constants and get

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) \propto \kappa_u^{\frac{n-1}{2} + \alpha_u - 1} \exp \big(- \kappa_u (\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u})\big), $$

which we recognize to be the density of a gamma-distributed variable with parameters $\frac{n-1}{2}+\alpha_u$ and $\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u}$.

We do the same for $\kappa_v$ 

$$p(\kappa_v \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_u, \mathbf{y}) \propto \kappa_v^{\frac{n}{2} + \alpha_v - 1} \exp \big(- \kappa_v (\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u}))\big),$$

which we recognize to be the density of a gamma-distributed variables with parameters $\frac{n}{2}+\alpha_v$ and $\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u})$.

We do the same for $\mathbf{u}$

$$p(\mathbf{u} \lvert \boldsymbol{\eta},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{\kappa_v}{2} \mathbf{u}^T \mathbf{u} - \frac{\kappa_u}{2} \mathbf{u}^T R \mathbf{u}\big) \\
= \exp(\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{1}{2}\mathbf{u}^T (\kappa_v I + \kappa_u R) \mathbf{u} \big),$$

which we recognize to be a normal density in canonical form with $\mathbf{b} = \kappa_v \boldsymbol{\eta}$ and $A = \kappa_v I + \kappa_u R$.

We do the same for $\boldsymbol{\eta}$

$$p(\boldsymbol{\eta} \lvert \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{\kappa_v}{2}\boldsymbol{\eta}^T \boldsymbol{\eta} + \sum_i (y_i \eta_i - E_i \exp(\eta_i))\Big) \\
= \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{1}{2}\boldsymbol{\eta}^T (\kappa_v I) \boldsymbol{\eta} + \boldsymbol{\eta}^T\mathbf{y} - \exp(\boldsymbol{\eta})^T\mathbf{E}\Big)$$

Using the Taylor expansion of $\mathbf{y}^T \boldsymbol{\eta} - \exp(\boldsymbol{\eta})^T \mathbf{E}$ we described in b), we may insert this into $p(\boldsymbol{\eta} \lvert \cdot )$ to create the approximation $q(\boldsymbol{\eta}\lvert \mathbf{z}, \cdot)$,

$$ q(\boldsymbol{\eta} \lvert \mathbf{z}, \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\boldsymbol{\eta}^T(\kappa_v\mathbf{u} + \mathbf{b}) -\frac{1}{2}\boldsymbol{\eta}^T (\kappa_v I + \text{diag}(\mathbf{c})) \boldsymbol{\eta} \big ),$$

where $\mathbf{b} = (b_1, \dots, b_n)^T$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$, $\mathbf{c} = (c_1, \dots, c_n)$, $c_i = E_i \exp(z_i)$, and $\text{diag}(\mathbf{c})$ is the matrix with the elements of $\mathbf{c}$ on the diagonal and zeros elsewhere. 

We observe that $q(\boldsymbol{\eta} \lvert \cdot)$ is a normal density written in canonical form and thus easily sampled from. We also observe that as the $\exp(\cdot)$ is a continuous transform, $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ will converge to $p(\boldsymbol{\eta} \lvert \cdot)$ as $\mathbf{z}$ gets increasingly close to $\boldsymbol{\eta}$. Thus, for a reasonably good $\mathbf{z}$,  $q(\boldsymbol{\eta} \lvert \mathbf{z})$ will be a good choice for a proposal density for $p(\boldsymbol{\eta} \lvert \cdot)$. 

#Exercise 2

We implement a Gibbs sampling algorithm with individual parameters updates using the full conditional for $\kappa_u$, $\kappa_v$ and $\mathbf{u}$. We update $\boldsymbol{\eta}$ using Metropolis-Hastings with $q(\boldsymbol{\eta} \lvert 
\mathbf{z}, \cdot)$ as the proposal density. We set $\mathbf{z} = \boldsymbol{\eta}^{(m-1)}$

```{r}
head(Oral)
```


```{r}
# Setup

load("tma4300_ex2_Rmatrix.Rdata")
n = length(Oral$SMR)
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01
```


```{r}
sample_u = function(kappa_v, kappa_u, eta){
  #Samples from the canonical normal
  b = kappa_v * eta
  A = kappa_v * diag(n) + kappa_u * R
  res = rmvnorm.canonical(1, b, A)
  return(t(res))
}

sample_eta = function(kappa_v, u, z){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_arg_sampler = kappa_v * u + b_taylor
  A_arg_sampler = kappa_v * diag(n) + diag(c_taylor)
  
  return(as.vector(rmvnorm.canonical(1, b_arg_sampler, A_arg_sampler)))
}

log_full_conditional_eta = function(eta, kappa_v, u, log=T){
  res = kappa_v * eta %*% u - 1/2 * t(eta) %*% (kappa_v * diag(n)) %*% eta + eta%*% Oral$Y - t(exp(eta))%*%Oral$E
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

log_full_conditional_eta_approx = function(eta, z, u, kappa_v,  log=T){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_canonical = kappa_v * u + b_taylor
  A_canonical = kappa_v * diag(n) + diag(c_taylor)
  
  res = eta %*% b_canonical - 1/2 * t(eta) %*% A_canonical %*% eta
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

#Set start values
kappau_start = alpha_u / beta_u
kappav_start = alpha_v / beta_v
u_start = seq(from=1, to=1, length.out = length(Oral$SMR))
eta_start = log(Oral$SMR)

#Set number of iterations
N_iter = 1000

u = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
eta = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
kappau = vector(mode = "double", length = N_iter)
kappav = vector(mode = "double", length = N_iter)
updated_eta = vector(length = N_iter)
times = vector(mode = "double", length = N_iter)


u[1,] = u_start
eta[1,] = eta_start
kappau[1] = kappau_start
kappav[1] = kappav_start



#Run the MCMC algorithm
for (i in 2:N_iter) {
  t0 = as.numeric(Sys.time())
    #Sample kappau
  kappau[i] = rgamma(1, (n-1)/2+alpha_u, beta_u + 1/2 * t(u[i-1,])%*%R%*%u[i-1,])
  
  #Sample kappav
  kappav[i] = rgamma(1, n/2 + alpha_v, beta_v + 1/2 * t(eta[i-1,] - u[i-1,])%*%(eta[i-1,] - u[i-1,]))
  
  #Sample v
  #cat(kappav[i], kappau[i], eta[i-1,])
  res = sample_u(kappav[i], kappau[i], eta[i-1,])
  u[i, ] = res
  
  #Sample eta
  z = eta[i-1,]
  eta_prop = sample_eta(kappav[i], u[i,], z)
  
  logp1 = log_full_conditional_eta(eta_prop, kappav[i], u[i,])
  logp2 = log_full_conditional_eta(z, kappav[i], u[i,])
  logp3 = log_full_conditional_eta_approx(z, eta_prop, u[i,], kappav[i])
  logp4 = log_full_conditional_eta_approx(eta_prop, z, u[i,], kappav[i])
  
  log_alpha = min(0, logp1 - logp2 + logp3 - logp4)
  #cat(log_alpha, ", ")
  r = runif(1)
  #cat(log(r) - log_alpha, ", ")
  if (log(r) < log_alpha){
    updated_eta[i] = TRUE
    eta[i,] = eta_prop
  }
  else{
    eta[i,] = eta[i-1,]
  }
  times[i] = as.numeric(Sys.time()) - t0 
  t0 = as.numeric(Sys.time())
}
```


```{r}
post_mean_eta = apply(eta[800:1000,], 2, mean)
post_mean_eta

post_mean_u = apply(u[800:1000,], 2, mean)

#germany.plot(post_mean_eta, col=col, legend=TRUE)
germany.plot(post_mean_u, col=col, legend=TRUE)
germany.plot(post_mean_eta, col=col, legend=TRUE)
```
```{r}

```

