--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 2, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(ggplot2)
library(coda)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```


```{r, echo = F, eval = T}
library(spam)         # load the data
attach(Oral)          # allow direct referencing to Y and E# load some libraries to generate nice map plots
library(fields, warn.conflict=FALSE)
library(colorspace)
col <- diverge_hcl(8)      # blue - red
# use a function provided by spam to plot the map together with the mortality rates
germany.plot(Oral$Y/Oral$E, col=col, legend=TRUE)
```

We wish to carry out a spatial analysis on mortality rates of oral cavity cancer in males in Germany during a 5-year period, 1986–1990.

We assume that the observed counts are conditionally independent Poisson

$$ \lambda_i \lvert \eta_i \sim Poisson(E_i \exp{\eta_i}) $$
The log relative risk $\boldsymbol{\eta} = (\eta_1, \dots, \eta_n)^T$ is decomposed into

$$ \boldsymbol{\eta} =  \mathbf{u} + \mathbf{v} $$, 

where the component $\mathbf{u} = (u_1, \dots, u_n)$ is spatially structured with smoothing parameter $\kappa_u$. The component $\mathbf{v} = (v_1, \dots, v_n)$ is unstructured white noise with precision parameter $\kappa_v$, i.e. $\mathbf{v} \sim N(\mathbf{0}, \kappa_v^{-1} I)$

For $\mathbf{u}$, we assume that neighboring districts are more similar than distant districts. We define that two districts are neighbours if they share a common border. $\mathbf{u}$ then becomes an intrinsic Gaussian Markov random field with density

$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \bigg (  \frac{-\kappa_u}{2}\sum_{i\sim j}{(u_i - u_j)^2}  \bigg)$$
We can define $R$ as the matrix
$$ R_{ij} = \begin{cases} n_i, \quad i = j \\
-1, \quad i \sim j \\
0, \quad \text{otherwise}. \end{cases}$$
and get
$$p(\mathbf{u} \lvert \kappa_u) \propto \kappa_u^{(n-1)/2}\exp \Big (  \frac{-\kappa_u}{2}
\mathbf{u}^T R \mathbf{u}\Big)$$



Where $n_i$ is the number of neighboring districts of district $i$ and $i \sim j$ denotes that district $i$ is a neighboring district of district $j$. The distribution of $\boldsymbol{\eta}$, conditional on the spatial component $\mathbf{u}$ and $\kappa_v$, is 

$$ \boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v \sim \mathcal{N}(\mathbf{u}, \kappa^{-1} I).$$

The precision terms $\kappa_u$ and $\kappa_v$ are assigned Gamma priors

$$ \kappa_u \sim \text{Gamma}(\alpha_u, \beta_u), \\
\kappa_v \sim \text{Gamma}(\alpha_v, \beta_v).$$

With $\alpha_u = \alpha_v = 1$ and $\beta_u = \beta_v = 0.01$. 

We wish to analyse the data and its underlying spatial structure by implementing a Gibbs sampler with individual parameter  updates based on the full conditional  distributions. 

# 1) Derivations

## a)

We start by computing the joint distribution from likelihood and prior distributions.

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \cdot p(\mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
 = p(\mathbf{y} \lvert \boldsymbol{\eta} ) p(\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v) p(\kappa_v)p(\mathbf{u} \lvert \kappa_u )p(\kappa_u)    \\
 $$
Inserting the Gamma density for $\kappa_u$ and $\kappa_v$, the normal density for $\mathbf{u}\lvert \kappa_u$ and the normal density for $\boldsymbol{\eta} \lvert \mathbf{u}, \kappa_v$ we get 


\begin{align*}
&p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto\\
&p(\mathbf{y} \lvert \boldsymbol{\eta} ) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big)
\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u).
\end{align*}

Inserting the Poisson density of the data $\mathbf{y}$, we get 

\begin{align*}
p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto &\Big(\prod_{i=1}^n \exp(\eta_i)^{y_i} \exp(-E_i \exp(\eta_i))\Big) \kappa_v^{n/2}\exp \Big (-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u})\Big) \cdot\\
&\kappa_v^{\alpha_v - 1} \exp(-\beta_v \kappa_v)
\kappa_u^{(n-1)/2} \exp \big(-\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}\big)
\kappa_u^{\alpha_u - 1} \exp(-\beta_u \kappa_u),
\end{align*}

which we can rewrite to 

$$ p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \\
\propto \kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}
-\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) + \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).$$

The posterior distribution of the parameters $p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$ is proportional to the joint distribution, giving us that 

\begin{align*} 
&p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y}) \propto p(\mathbf{y}, \mathbf{u}, \boldsymbol{\eta}, \kappa_u, \kappa_v) \propto \\
&\kappa_u^{(n-1)/2 + \alpha_u - 1}\kappa_v^{n/2 + \alpha_v - 1}\exp \Big(-\beta_u \kappa_u -\beta_v \kappa_v -\frac{\kappa_v}{2}(\boldsymbol{\eta} - \mathbf{u})^T(\boldsymbol{\eta} - \mathbf{u}) - \frac{\kappa_u}{2}\mathbf{u}^T R \mathbf{u}+ \sum_i (y_i \eta_i - E_i \exp(\eta_i) )\Big  ).
\end{align*}

## b)
Due to the non-normality, sampling from the posterior distribution will require a Metropolis–Hastings step. To obtain a proposal distribution that is easy to sample from, here a Gaussian, we note that we can approximate the function 

$$f(\eta_i) = y_i \eta_i - E_i \exp(\eta_i)$$

by its 2nd order Taylor expansion, $\tilde{f}(\eta_i)$, around a point $z_i$. The approximation can be written as, 

\begin{align*}\tilde{f}(\eta_i) &= y_i z_i - E_i \exp(z_i) + (y_i - E_i \exp(z_i)) (\eta_i - z_i) - E_i \exp(z_i) \frac{(\eta_i - z_i)^2}{2} \\
&= E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2}) + \eta_i (y_i + E_i \exp(z_i)(z_i - 1)) - \frac{\eta_i^2}{2} E_i \exp (z_i) \\
&= a_i + b_i \eta_i - \frac{c_i}{2}\eta_i^2,
\end{align*}
where $a_i = E_i \exp(z_i)(z_i - 1 - \frac{z_i^2}{2})$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$ and $c_i = E_i \exp (z_i)$.

## c)

The full conditional of $\kappa_u$ can be computed by

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) = \frac{p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})}{ \int p(\boldsymbol{\eta}, \mathbf{u}, \kappa_v, \kappa_u \lvert \mathbf{y})d\kappa_u} \propto  p(\boldsymbol{\eta}, \mathbf{u},  \kappa_u, \kappa_v \lvert \mathbf{y})$$
We use the fact that any factor in the posterior not containing $\kappa_u$ are now constants and get

$$ p(\kappa_u \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_v, \mathbf{y}) \propto \kappa_u^{\frac{n-1}{2} + \alpha_u - 1} \exp \big(- \kappa_u (\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u})\big), $$

which we recognize to be the density of a gamma-distributed variable with parameters $\frac{n-1}{2}+\alpha_u$ and $\beta_u + \frac{1}{2}\mathbf{u}^T R \mathbf{u}$.

We follow along the same lines for $\kappa_v$, and get 

$$p(\kappa_v \lvert \boldsymbol{\eta}, \mathbf{u},  \kappa_u, \mathbf{y}) \propto \kappa_v^{\frac{n}{2} + \alpha_v - 1} \exp \big(- \kappa_v (\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u}))\big),$$

which we recognize to be the density of a gamma-distributed variables with parameters $\frac{n}{2}+\alpha_v$ and $\beta_v + \frac{1}{2}(\boldsymbol{\eta} - \mathbf{u})^T (\boldsymbol{\eta} - \mathbf{u})$.

Again, we do the same for $\mathbf{u}$, and get

$$p(\mathbf{u} \lvert \boldsymbol{\eta},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{\kappa_v}{2} \mathbf{u}^T \mathbf{u} - \frac{\kappa_u}{2} \mathbf{u}^T R \mathbf{u}\big) \\
= \exp(\kappa_v \boldsymbol{\eta}^T \mathbf{u} - \frac{1}{2}\mathbf{u}^T (\kappa_v I + \kappa_u R) \mathbf{u} \big),$$

which we recognize to be a normal density in canonical form with $\mathbf{b} = \kappa_v \boldsymbol{\eta}$ and $A = \kappa_v I + \kappa_u R$.

Finally, we repeat the procedure for $\boldsymbol{\eta}$, and get

$$p(\boldsymbol{\eta} \lvert \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{\kappa_v}{2}\boldsymbol{\eta}^T \boldsymbol{\eta} + \sum_i (y_i \eta_i - E_i \exp(\eta_i))\Big) \\
= \exp \Big (\kappa_v\boldsymbol{\eta}^T\mathbf{u} -\frac{1}{2}\boldsymbol{\eta}^T (\kappa_v I) \boldsymbol{\eta} + \boldsymbol{\eta}^T\mathbf{y} - \exp(\boldsymbol{\eta})^T\mathbf{E}\Big)$$

Using the Taylor expansion of $\mathbf{y}^T \boldsymbol{\eta} - \exp(\boldsymbol{\eta})^T \mathbf{E}$, as described in b), we may insert this into $p(\boldsymbol{\eta} \lvert \cdot )$ to create the approximation $q(\boldsymbol{\eta}\lvert \mathbf{z}, \cdot)$,

$$ q(\boldsymbol{\eta} \lvert \mathbf{z}, \mathbf{u},  \kappa_u, \kappa_v, \mathbf{y}) \propto \exp \big (\boldsymbol{\eta}^T(\kappa_v\mathbf{u} + \mathbf{b}) -\frac{1}{2}\boldsymbol{\eta}^T \big(\kappa_v I + \text{diag}(\mathbf{c})\big) \boldsymbol{\eta} \big ),$$

where $\mathbf{b} = (b_1, \dots, b_n)^T$, $b_i = y_i + E_i \exp(z_i)(z_i - 1)$, $\mathbf{c} = (c_1, \dots, c_n)^T$, $c_i = E_i \exp(z_i)$, and $\text{diag}(\mathbf{c})$ is the matrix with the elements of $\mathbf{c}$ on the diagonal and zeros elsewhere. 

We observe that $q(\boldsymbol{\eta} \lvert \cdot)$ is a normal density written in canonical form and thus easily sampled from. We also observe that as the $\exp(\cdot)$ is a continuous transform, $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ will converge to $p(\boldsymbol{\eta} \lvert \cdot)$ as $\mathbf{z}$ gets increasingly close to $\boldsymbol{\eta}$. Thus, for a reasonably good $\mathbf{z}$,  $q(\boldsymbol{\eta} \lvert \mathbf{z})$ will be a good choice for a proposal density for $p(\boldsymbol{\eta} \lvert \cdot)$. 

# 2) Implementation of the MCMC sampler 

We implement a Gibbs sampling algorithm with individual parameter updates using the full conditional for $\kappa_u$, $\kappa_v$ and $\mathbf{u}$. We update $\boldsymbol{\eta}$ using Metropolis-Hastings with $q(\boldsymbol{\eta} \lvert \mathbf{z}, \cdot)$ as the proposal density. We set $\mathbf{z} = \boldsymbol{\eta}^{(m-1)}$, and do $N = 10000$ iterations.

```{r, echo = T, eval = T}
#head(Oral)
```


```{r, echo = T, eval = T}
# Setup

load("tma4300_ex2_Rmatrix.Rdata")
n = length(Oral$SMR)
alpha_u = 1
alpha_v = 1
beta_u = 0.01
beta_v = 0.01
```


```{r, echo = T, eval = T}
source("dmvnorm.R")

sample_u = function(kappa_v, kappa_u, eta){
  #Samples from the canonical normal
  b = kappa_v * eta
  A = kappa_v * diag.spam(n) + kappa_u * R
  res = rmvnorm.canonical(1, b, A)
  return(t(res))
}

sample_eta = function(kappa_v, u, z){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_arg_sampler = kappa_v * u + b_taylor
  A_arg_sampler = kappa_v * diag.spam(n) + diag(c_taylor)
  
  return(as.vector(rmvnorm.canonical(1, b_arg_sampler, A_arg_sampler)))
}

log_full_conditional_eta = function(eta, kappa_v, u, log=T){
  res = kappa_v * eta %*% u - 1/2 * t(eta) %*% (kappa_v * diag.spam(n)) %*% eta + eta%*% Oral$Y - t(exp(eta))%*%Oral$E
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

log_full_conditional_eta_approx = function(eta, z, u, kappa_v,  log=T){
  b_taylor = Oral$Y + Oral$E * exp(z) * (z - 1)
  c_taylor = Oral$E * exp(z)
  
  b_canonical = kappa_v * u + b_taylor
  A_canonical = kappa_v * diag.spam(n) + diag.spam(c_taylor)
  
  #SKRIVE INN DMVNORM HER
  #res = eta %*% b_canonical - 1/2 * t(eta) %*% A_canonical %*% eta
  res = dmvnorm.canonical(eta, b_canonical, A_canonical)
  
  if (log){
    return(res)
  }
  else {
    return(exp(res))
  }
}

#Set number of iterations
N_iter = 10000

#Allocate vectors and arrays 
u = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
eta = matrix(data= NA, nrow = N_iter, ncol = length(Oral$Y))
kappau = vector(mode = "double", length = N_iter)
kappav = vector(mode = "double", length = N_iter)
updated_eta = vector(length = N_iter)
accept_prob = vector(length = N_iter)
times = vector(mode = "double", length = N_iter)

#Set start values
kappau_start = alpha_u / beta_u
kappav_start = alpha_v / beta_v
u_start = rep(0, length(Oral$SMR))
eta_start = log(Oral$SMR)

u[1,] = u_start
eta[1,] = eta_start
kappau[1] = kappau_start
kappav[1] = kappav_start



#Run the MCMC algorithm
for (i in 2:N_iter) {
  t0 = as.numeric(Sys.time())
    #Sample kappau
  kappau[i] = rgamma(1, (n-1)/2+alpha_u, beta_u + 1/2 * t(u[i-1,])%*%R%*%u[i-1,])
  
  #Sample kappav
  kappav[i] = rgamma(1, n/2 + alpha_v, beta_v + 1/2 * t(eta[i-1,] - u[i-1,])%*%(eta[i-1,] - u[i-1,]))
  
  #Sample u
  #cat(kappav[i], kappau[i], eta[i-1,])
  res = sample_u(kappav[i], kappau[i], eta[i-1,])
  u[i, ] = res
  
  #Sample eta
  z = eta[i-1,]
  eta_prop = sample_eta(kappav[i], u[i,], z)
  
  logp1 = log_full_conditional_eta(eta_prop, kappav[i], u[i,])
  logp2 = log_full_conditional_eta(z, kappav[i], u[i,])
  logp3 = log_full_conditional_eta_approx(z, eta_prop, u[i,], kappav[i])
  logp4 = log_full_conditional_eta_approx(eta_prop, z, u[i,], kappav[i])
  
  log_alpha = min(0, logp1 - logp2 + logp3 - logp4)
  accept_prob[i] = exp(log_alpha)
  
  #cat("log1",logp1, " log", logp2, "log3",logp3, " log4", logp4)
  if(!(i%%100)){
    cat(i, end=" ")
  }
  #cat("logdens: ", log_alpha, ", ")
  r = runif(1)
  #cat(log(r) - log_alpha, ", ")
  if (log(r) < log_alpha){
    updated_eta[i] = TRUE
    eta[i,] = eta_prop
  }
  else{
    eta[i,] = eta[i-1,]
  }
  times[i] = as.numeric(Sys.time()) - t0 
}
```
We note that the code takes `r round(sum(times))` seconds, or `r round(sum(times)/60,1)` minutes.

# Exercise 3 (Convergence diagnostics)

We now check a few diagnostic summaries for the precision parameters $\kappa_u$ and $\kappa_v$. We also do the same for three randomly chosen components of $\mathbf{u}$ and $\mathbf{v}$.

First, we show a trace plot of $\kappa_u$. The values in the burn-in period are really large, so we display both a plot of the whole chain, and a plot where the first 500 values are removed.
```{r, echo = T, eval = T, out.width = "50%"}
df1 = data.frame(kappa_u = kappau, N = 1:N_iter)
ggplot(df1, aes(x = N, y = kappa_u)) + geom_point()
df12 = data.frame(kappa_u = kappau[500:N_iter], N = 500:N_iter)
ggplot(df12, aes(x = N, y = kappa_u)) + geom_point()
```
From the second plot, we suspect that the burn-in period might be somewhat over 500 iterations.

We then display an autocorrelation plot for $\kappa_u$.
```{r, echo = T, eval = T, out.width = "50%", fig.align = "center"}
acf(kappau, lag.max = 60)
```
From the plot, we see that we need to use only every $\sim 45$^th^ iteration to get a correlation which is not statistically significant.

SKRIV NOE OM GEWEKE
```{r, echo = T, eval = T}
gw = geweke.diag(kappau)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```



We then move on to $\kappa_v$, and display both a plot of the whole chain, and a plot where the first 500 values are removed.
```{r, echo = T, eval = T, out.width = "50%"}
df2 = data.frame(kappa_v = kappav, N = 1:N_iter)
ggplot(df2, aes(x = N, y = kappa_v)) + geom_point()
df21 = data.frame(kappa_v = kappav[500:N_iter], N = 500:N_iter)
ggplot(df21, aes(x = N, y = kappa_v)) + geom_point()
```
Also here, we suspect that the burn-in period might be somewhat over 500 iterations.

We then display an autocorrelation plot for $\kappa_v$.
```{r, echo = T, eval = T, out.width = "50%"}
acf(kappav)
acf(kappav, lag.max = 1000)
```
The plot shows that the samples of $\kappa_v$ are highly correlated.


SKRIV OM GEWEKE
```{r, echo = T, eval = T}
gw = geweke.diag(kappav)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```


We then draw three random components of $\mathbf{u}$ and $\boldsymbol \eta$ and plot trace plots, autocorrelation functions and use the function `geweke.diag()` to check for convergence.
```{r, echo = T, eval = T, out.width = "50%", fig.align = "center"}
random_indexes = sort(sample(1:length(Oral$Y), 3, replace=F))
df4 = data.frame()
for (i in 1:3){
  index = random_indexes[i]
  df = data.frame(u = u[, index], N = 1:N_iter, shape = as.factor(i), col=as.factor(index))
  df4 = rbind(df4, df)
}
ggplot(df4, aes(x = N, y = u, shape = shape, col=col)) + geom_point() + scale_shape(solid=F) + guides(shape = F)
```
The trace plot for three components of $\mathbf{u}$ looks correct. 


```{r, echo = T, eval = T, out.width = "33%"}
df5 = data.frame(u = u[,random_indexes])
#acf(df5, lag.max = 200)

acf(u[,random_indexes[1]], lag.max = 200)
acf(u[,random_indexes[2]], lag.max = 200)
acf(u[,random_indexes[3]], lag.max = 200)
```
The three components of $\mathbf{u}$ show varying degree of autocorrelation, but generally, all three components are highly autocorrelated.

SKRIV OM GEWEKE
```{r, echo = T, eval = T}
gw = geweke.diag(df5)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```


```{r, echo = T, eval = T, out.width = "50%", fig.align = "center"}
#random_indexes = sort(sample(1:length(Oral$Y), 3, replace=F))
df4 = data.frame()
for (i in 1:3){
  index = random_indexes[i]
  df = data.frame(eta = eta[, index], N = 1:N_iter, shape = as.factor(i), col=as.factor(index))
  df4 = rbind(df4, df)
}
ggplot(df4, aes(x = N, y = eta, shape = shape, col=col)) + geom_point() + scale_shape(solid=F)+ guides(shape = F)
```

```{r, echo = T, eval = T, out.width = "33%"}
df7 = data.frame(eta = eta[,random_indexes])
#head(df7)
#acf(df7, lag.max = 200)

acf(eta[,random_indexes[1]], lag.max = 200)
acf(eta[,random_indexes[2]], lag.max = 200)
acf(eta[,random_indexes[3]], lag.max = 200)
```
Same as for the components of $\mathbf{u}$, the three components of $\boldsymbol \eta$ show varying degree of autocorrelation, but generally, all three components are highly autocorrelated.


SKRIV OM GEWEKE
```{r, echo = T, eval = T}
gw = geweke.diag(df7)
cat("zscores:", gw$z, "\n")
cat("pvals:", 2*(1-pnorm(abs(gw$z))))
```

# Exercise 4 (Effective sample size)

```{r}
library(coda)
ESS_kappau = effectiveSize(kappau)
ESS_kappav = effectiveSize(kappav)
```
```{r}
running_time = sum(times)

cat("kappau \n")
cat(running_time, "\n")
cat(ESS_kappau / running_time, "\n")

cat("kappav \n")
cat(running_time, "\n")
cat(ESS_kappav / running_time)
```


# Exercise 5 (Interpretation of results)


```{r, echo = T, eval = T, out.width = "50%"}
post_mean_eta = apply(eta[1000:10000,], 2, mean)
#post_mean_eta = apply(exp(eta[1000:10000,]), 2, mean)

post_median_eta = apply(exp(eta[1000:10000,]), 2, median)

post_mean_u = apply(exp(u[1000:10000,]), 2, mean)
post_median_u = apply(exp(u[1000:10000,]), 2, median)

#germany.plot(post_mean_eta, col=col, legend=TRUE)
germany.plot(post_median_u, col=col, legend=TRUE)
```

# Exercise 6 (Comparison of INLA and inclusion of covariate information)

```{r}
g = system.file("demodata/germany.graph", package="INLA")

```

